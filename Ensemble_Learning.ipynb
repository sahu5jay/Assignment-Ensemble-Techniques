{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efaaae3b-7b0a-400d-9d99-80bb033fbe05",
   "metadata": {},
   "source": [
    "Que-1 What is Ensemble Learning in machine learning? Explain the key idea behind it. \n",
    "Ans-1 Ensemble Learning is a machine learning technique where multiple models (called base or weak learners) are trained and then combined to make a final prediction. The main goal is to improve accuracy, stability, and generalization compared to using a single model.\n",
    "It reduces the dependiency of the result on only one model to several base model\n",
    "Individual errors cancel out\n",
    "Overall performance improves\n",
    "Overfitting and underfitting are reduced\n",
    "Ensemble learning is a technique where multiple machine learning models are combined to produce a better and more reliable prediction than a single model. The key idea is that aggregating diverse models reduces errors, improves accuracy, and enhances generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64372e93-f65d-4d77-94f2-22e265b030ae",
   "metadata": {},
   "source": [
    "Que-2 What is the difference between Bagging and Boosting?\n",
    "Ans-2 Bagging builds multiple independent models in parallel using different random subsets of the training data (with replacement) and then aggregates their predictions.\n",
    "Create multiple bootstrap samples from the dataset\n",
    "Train a separate model on each sample\n",
    "Combine predictions using voting (classification) or averaging (regression)\n",
    "Best suited for:High-variance models (e.g., Decision Trees)\n",
    "\n",
    "Boosting-Boosting builds models sequentially, where each new model focuses on correcting the errors made by the previous models.\n",
    "Train an initial weak model\n",
    "Increase the weight of misclassified data points\n",
    "Train the next model to focus more on these difficult cases\n",
    "Combine all models using weighted voting\n",
    "Best suited for: High-bias models, Complex patterns in data\n",
    "Ex- AdaBoost, Gradient Boosting, XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d213d69-9809-4676-9d57-faa11c1a252a",
   "metadata": {},
   "source": [
    "Que-3 What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
    "Ans-3 Bootstrap sampling is a statistical technique where we create multiple new datasets by randomly sampling from the original dataset with replacement.\n",
    "Each bootstrap sample has the same size as the original dataset\n",
    "Some observations may appear multiple times\n",
    "Some observations may be left out (called out-of-bag samples)\n",
    "\n",
    "In Bagging methods like Random Forest, bootstrap sampling is the core mechanism that introduces diversity among models.\n",
    "How it works in Random Forest:\n",
    "From the original dataset, create many bootstrap samples\n",
    "Train one decision tree per bootstrap sample\n",
    "Each tree learns different patterns\n",
    "Final prediction is made using majority voting (classification) or averaging (regression)\n",
    "\n",
    "Bootstrap sampling is a method of creating multiple datasets by sampling the original data with replacement. In Bagging methods like Random Forest, it ensures that each tree is trained on a different subset of data, introducing diversity among models, reducing variance, and improving overall prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911955d-53b3-4411-813a-b1ef7987d36f",
   "metadata": {},
   "source": [
    "Que-4 What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models? \n",
    "Ans-4 In bootstrap sampling, data is sampled with replacement to train each model (tree).\n",
    "Because of this, not all observations are selected in a given bootstrap sample.\n",
    "The data points not selected for training a particular model are called Out-of-Bag (OOB) samples.\n",
    "We randomly draw 100 rows with replacement\n",
    "How OOB Samples Arise\n",
    "Some rows repeat\n",
    "Some rows are never chosen\n",
    "Those unchosen rows = OOB samples\n",
    "How OOB Score Is Used to Evaluate Ensemble Models\n",
    "Out-of-Bag samples are the data points that are not included in a bootstrap sample used to train an individual model. The OOB score evaluates ensemble models by predicting each data point using only the models for which it was OOB and then comparing the prediction with the true value. It provides an efficient and unbiased estimate of model performance without needing a separate validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36edfcf9-10c3-4851-9a1b-5ce70c1d0b45",
   "metadata": {},
   "source": [
    "Que-5 Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
    "Ans-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2caff733-0644-45db-99b7-91ececa3c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Important Features:\n",
      "                 feature  feature_importance\n",
      "23            worst area            0.139357\n",
      "27  worst concave points            0.132225\n",
      "7    mean concave points            0.107046\n",
      "20          worst radius            0.082848\n",
      "22       worst perimeter            0.080850\n"
     ]
    }
   ],
   "source": [
    "# <!-- Que-6 Write a Python program to:  -->\n",
    "# <!-- ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()  -->\n",
    "# <!-- ● Train a Random Forest Classifier  -->\n",
    "# <!-- ● Print the top 5 most important features based on feature importance scores.  -->\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "data = load_breast_cancer()\n",
    "x, y = data.data, data.target\n",
    "feature_name = data.feature_names\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(x, y)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_name,\n",
    "    'feature_importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "top_5_features = feature_importance_df.sort_values(by='feature_importance',ascending=False).head(5)\n",
    "\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86bfb77a-02e8-4a29-84de-50fc95e3dc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy score: 1.0\n",
      "Bagging Classifier accuracy score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Que-7 Write a Python program to: \n",
    "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset \n",
    "# ● Evaluate its accuracy and compare with a single Decision Tree \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "x, y = data.data, data.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "model_decision = DecisionTreeClassifier()\n",
    "model_decision.fit(x_train, y_train)\n",
    "y_pred_decision = model_decision.predict(x_test)\n",
    "print(f\"Decision Tree accuracy score: {accuracy_score(y_pred_decision, y_test)}\")\n",
    "\n",
    "model_Bagging = BaggingClassifier(estimator=DecisionTreeClassifier(),n_estimators=100, random_state=42)\n",
    "model_Bagging.fit(x_train, y_train)\n",
    "y_pred_Bagging = model_Bagging.predict(x_test)\n",
    "print(f\"Bagging Classifier accuracy score: {accuracy_score(y_pred_Bagging, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0c7d026-006d-4432-bf03-3a0e5507d482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "15 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'n_estimators' parameter of RandomForestClassifier must be an int in the range [1, inf). Got None instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan 0.93333333 0.93333333 0.94285714        nan 0.93333333\n",
      " 0.93333333 0.94285714        nan 0.93333333 0.93333333 0.94285714]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to: \n",
    "# ● Train a Random Forest Classifier \n",
    "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV \n",
    "# ● Print the best parameters and final accuracy \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "x = data.data\n",
    "y = data.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 42, test_size= 0.3)\n",
    "\n",
    "hyper_para = {\n",
    "    'n_estimators': [None, 5, 10, 20],\n",
    "    'max_depth': [50, 100, 200]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator= model, param_grid= hyper_para, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_rf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "356bf7f5-9dc6-4f2e-885c-d432b579a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Bagging:  0.2568358813508342\n",
      "Mean Squared Error random:  0.25650512920799395\n"
     ]
    }
   ],
   "source": [
    "# Write a Python program to: \n",
    "# ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset \n",
    "# ● Compare their Mean Squared Errors (MSE) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "\n",
    "x = data.data\n",
    "y = data.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 42, test_size = 0.3)\n",
    "\n",
    "bagging_regressor = BaggingRegressor(estimator= DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
    "model_bagging = bagging_regressor.fit(x_train, y_train)\n",
    "y_pred_bagging  = model.predict(x_test)\n",
    "# print(\"Mean Squared Error : \", mean_squared_error(y_pred_bagging, y_test))\n",
    "\n",
    "random_model = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "random_model.fit(x_train, y_train)\n",
    "y_pred_random = random_model.predict(x_test)\n",
    "print(\"Mean Squared Error Bagging: \", mean_squared_error(y_pred_bagging, y_test))\n",
    "print(\"Mean Squared Error random: \", mean_squared_error(y_pred_random, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fa2ea-a6d2-4925-b1a8-30ae9ec70e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model Choice\n",
    "Use Bagging / Random Forest when decision trees overfit (high variance).\n",
    "Use Boosting (XGBoost / Gradient Boosting) when simple models underfit (high bias).\n",
    "Overfitting Control\n",
    "Limit tree depth, increase minimum samples per leaf.\n",
    "Use early stopping (for boosting).\n",
    "Apply cross-validation and OOB error (for bagging).\n",
    "Base Models\n",
    "Prefer Decision Trees as base learners.\n",
    "Shallow trees for boosting, deeper trees for bagging.\n",
    "Model Evaluation\n",
    "Use Stratified K-Fold Cross-Validation.\n",
    "Focus on ROC-AUC, Precision, Recall, F1-score, not just accuracy.\n",
    "Why Ensembles\n",
    "Reduce variance and bias.\n",
    "Capture complex transaction patterns.\n",
    "Provide stable, reliable predictions crucial for financial risk decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
